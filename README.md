
<p align="center">
  <img src="assets/demo.png" alt="minillm demo" width="800"/>
</p>

# minillm â€” local llm + rag demo

this repo shows how to **train your own mini-llm**, add a **retrieval-augmented generation (rag) pipeline**, and serve a **pretrained hf model** with a simple web ui.  
tested on macbook pro m1. lightweight + educational.

---

## features
- **train from scratch** â†’ tiny gpt-like model on your own dataset  
- **rag** â†’ index + search docs with embeddings + faiss (vector search)  
- **pretrained model** â†’ integrate huggingface models (`CraneAILabs/ganda-gemma-1b` by default)  
- **web ui** â†’ minimal chat-like interface (`web/index.html`)  
- **fastapi server** â†’ backend serving, easy to extend  

---

## repo structure
minillm/
â”œâ”€â”€ src/ # training + serving code <br> </br>
â”‚ â”œâ”€â”€ server_hf.py # fastapi server for pretrained model
â”‚ â”œâ”€â”€ server.py # (optional) server for scratch-trained model
â”‚ â”œâ”€â”€ train_minigpt.py # training loop for mini-llm
â”‚ â”œâ”€â”€ sample_minillm.py
â”‚ â”œâ”€â”€ model.py # gpt-like model definition
â”‚ â””â”€â”€ tools/ # helpers: tokenizer training, binify, corpus downloader
â”‚
â”œâ”€â”€ rag/ # retrieval-augmented generation pipeline
â”‚ â”œâ”€â”€ ingest.py # chunk + embed documents
â”‚ â”œâ”€â”€ build_index.py # build faiss/annoy index
â”‚ â”œâ”€â”€ search.py # query top-k passages
â”‚ â””â”€â”€ index/ # generated vector indexes (ignored in git)
â”‚
â”œâ”€â”€ web/ # minimal html/js ui
â”‚ â””â”€â”€ index.html
â”‚
â”œâ”€â”€ tokenizer/ # optional tokenizer artifacts
â”‚ â”œâ”€â”€ spm_bpe_16k.model
â”‚ â””â”€â”€ spm_bpe_16k.vocab
â”‚
â”œâ”€â”€ checkpoints/ # training checkpoints (ignored in git)
â”‚ â””â”€â”€ out-minillm/best.pt
â”‚
â”œâ”€â”€ pretrained/ # pointers for hf pretrained models
â”‚ â””â”€â”€ README_PRETRAINED.md
â”‚
â”œâ”€â”€ scripts/ # helpers for downloads + setup
â”‚ â””â”€â”€ download_weights.sh
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ README_RAG.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore


---

## ðŸš€ quickstart

### 1. setup environment
```bash
python3 -m venv ~/.venvs/minillm
source ~/.venvs/minillm/bin/activate
pip install -r requirements.txt
2. run the pretrained server

uvicorn src.server_hf:app --reload --port 8000
open browser â†’ http://127.0.0.1:8000
youâ€™ll see the chat ui running locally.

3. run rag indexing
see README_RAG.md for details.

4. train from scratch

python src/train_minigpt.py
checkpoints saved to checkpoints/out-minillm/.

handling large files
model checkpoints (.pt, .bin), rag indexes (.faiss, .pkl) â†’ not committed (see .gitignore)

download them using scripts/download_weights.sh

pretrained model instructions in README_PRETRAINED.md


license & credits

training code â†’ inspired by nanoGPT & HF examples
pretrained â†’ CraneAILabs/ganda-gemma-1b
rag â†’ faiss + sentence-transformers
ui â†’ minimal html/css/js
licensed under mit. datasets/models used must follow their respective licenses.
